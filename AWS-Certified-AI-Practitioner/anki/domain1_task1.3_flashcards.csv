"Which AWS service is specifically designed to help with the data preparation and feature engineering phase of the ML lifecycle by providing a visual, code-free interface?","Amazon SageMaker Data Wrangler","Amazon S3","Amazon SageMaker Model Monitor","Amazon EC2","SageMaker Data Wrangler provides a unified visual interface to select, cleanse, explore, and visualize data, simplifying the process of data preparation and feature engineering."
"A team has successfully trained an ML model and now needs to automate the entire workflow from data preparation to model deployment for continuous retraining. Which SageMaker feature is best suited for this MLOps task?","Amazon SageMaker Pipelines","Amazon SageMaker Studio","Amazon SageMaker Ground Truth","Amazon SageMaker JumpStart","SageMaker Pipelines is the purpose-built CI/CD service for machine learning, allowing you to create, automate, and manage end-to-end ML workflows."
"When evaluating a classification model's performance, which metric is most appropriate when you need a balance between Precision and Recall, especially if the dataset is imbalanced?","F1 Score","Accuracy","AUC (Area Under the Curve)","RMSE (Root Mean Square Error)","The F1 Score is the harmonic mean of Precision and Recall, providing a better measure of a model's performance on datasets where one class is much more frequent than the other."
"A developer needs to deploy a newly trained ML model. The model will be invoked infrequently with unpredictable traffic spikes. Which deployment method offers a pay-per-use model without requiring the developer to manage the underlying infrastructure?","A managed API service (like a SageMaker Serverless Inference endpoint)","A self-hosted API on an Amazon EC2 instance","Batch inferencing","Edge deployment","A managed API service, particularly a serverless one, is ideal for sporadic workloads. It automatically scales and you only pay for the compute time you use, abstracting away server management."
"What is the primary purpose of a validation set in the ML development lifecycle?","To tune model hyperparameters","To train the final model","To test the model's performance on unseen production data","To perform exploratory data analysis (EDA)","The validation set is used during the training phase to tune the model's hyperparameters (like learning rate) to prevent overfitting and improve generalization."
"A company is concerned that its ML model's performance is degrading over time in production due to changes in the incoming data. Which SageMaker feature should be used to detect this 'model drift'?","Amazon SageMaker Model Monitor","Amazon SageMaker Clarify","Amazon SageMaker Feature Store","Amazon SageMaker Automatic Model Tuning","SageMaker Model Monitor is designed to continuously monitor the quality of ML models in production and alert you to deviations like data drift or concept drift."
"What is a core principle of MLOps that helps manage technical debt and ensure reproducibility in machine learning projects?","Version control for data, code, and models","Focusing solely on business metrics","Using only open-source models","Manual deployment for all models","Versioning all components of the ML workflow (data, code, models, parameters) is a fundamental MLOps practice that ensures reproducibility, traceability, and the ability to roll back changes."
"Which AWS service acts as a centralized repository to store, organize, and share curated features for ML model training and inference across an organization?","Amazon SageMaker Feature Store","Amazon Comprehend","AWS Glue DataBrew","Amazon Kendra","SageMaker Feature Store is a purpose-built repository that makes it easy to store, retrieve, and share ML features, ensuring consistency between training and inference."
