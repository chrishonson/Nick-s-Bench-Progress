#separator:tab
#html:true
What is an Amazon VPC?	A logically-isolated virtual network in the AWS Cloud that you configure with your own CIDR block, subnets, route tables, and gateways.
Which resource lets instances reach the wider internet?	An **Internet Gateway (IGW)** attached to the VPC and referenced in a subnet’s route table.
What is the allowed CIDR range for a VPC?	You can choose any /16 to /28 block (65 536 – 16 IPs).
How many Availability Zones can a single subnet span?	Exactly **one**; a subnet is always confined to a single AZ.
Why can subnets in the same VPC route to each other automatically?	Every VPC’s main route table includes an implicit “local” route for the entire CIDR.
Security Group vs. Network ACL in one sentence?	"SGs are **stateful** and attach to ENIs; NACLs are **stateless** and apply to subnets.<br><div>**NACL stands for <b>Network Access Control List</b> — an optional, subnet-level firewall in an Amazon VPC that lets you allow <b>or</b> deny specific inbound and outbound traffic.</div><div><br></div><div><div>A Security Group is stateful because if you allow incoming traffic on a specific port, the outbound return traffic for that same connection is <strong>automatically allowed</strong>, regardless of any outbound rules you've configured.</div><div>Essentially, it ""remembers"" the connections that are initiated and permits the corresponding responses back out.</div></div>"
Default limit: how many security groups per ENI?	**5** (soft limit; can be raised).
What limitation exists with VPC peering?	No **transitive routing** — you must peer each VPC pair directly.
Which Route 53 record type maps a root domain to an ALB or S3 site?	An **Alias A/AAAA** record.<br><strong>CNAME Limitation:</strong> Standard DNS CNAME (Canonical Name) records cannot be used for a root domain. This is due to DNS specifications that state a CNAME record cannot coexist with other record types (like SOA or NS records) that are inherently present at the root of a domain.<br><br><strong>ALIAS Record to the Rescue:</strong> Amazon Route 53 provides a proprietary feature called an ALIAS record. This special type of record allows you to map your root domain (also known as the zone apex or naked domain) to specific AWS resources, including:
List the four components that come with a default VPC.	One VPC, a public subnet in every AZ, an Internet Gateway, and a main route table with 0.0.0.0/0 to the IGW.
What subnet-level feature filters traffic, separate from SGs?	**Network ACLs** with ordered, stateless rules.
What built-in DNS resolver exists at the .2 address?	**AmazonProvidedDNS** (Route 53 Resolver in each subnet).<br>When you create a Virtual Private Cloud (VPC) in AWS, it's automatically assigned a DNS server. This server is located at an IP address that is the <strong>base of your VPC's primary IPv4 CIDR block, plus two</strong>.
How do you privately access S3 without using the internet?	Create a **Gateway VPC endpoint** for S3 and update route tables.
When would you use a NAT Gateway instead of an IGW?	For private-subnet instances that need outbound internet access without exposing inbound ports.
Where do you place an Application Load Balancer?	In **public subnets**; its targets can live in private or public subnets.
what is the .com part of a url called	TLD: top level domain
what is the example part called in example.com	SLD: second level domain
what does a name server do	resolves DNS queries
what is a second level domain	example in example.com
what is the www part called	sub domian
in api.www.example.com what is the api part and everything after called&nbsp;	fully qualified domain name (FQDN)
cname vs alias	cname: only for non root domain (app.mydomain.com (subdomain))<br>alias: subdomain or root domain
can you set an alias for an ec2 dns name/target	no, Alais records are for AWS services that Route 53 automatically can track IP address changes of. It can't do that with EC2 because you can acheive the same thing already by creating an A record pointing to an Elastic IP for the EC2 instance.
What is Amazon S3?	Amazon Simple Storage Service is an object storage service offering scalability, data availability, security, and performance.
What are the basic storage units in S3 called?	Objects
Where are objects stored in S3?	In buckets. Think of buckets as containers for objects.
What is a unique identifier for an object within a bucket?	"<ul><li>The object key (or key name).</li>
</ul><div></div>"
How is data stored in S3 organized?	S3 uses a flat structure. However, you can use prefixes (similar to folders) for organizational purposes.
What are the key characteristics of S3?	Scalability, high availability (designed for 99.99% availability), durability (designed for 99.999999999% durability), and security.
Name some common S3 storage classes.	S3 Standard, S3 Intelligent-Tiering, S3 Standard-Infrequent Access (S3 Standard-IA), S3 One Zone-Infrequent Access (S3 One Zone-IA), S3 Glacier<sup></sup> Instant Retrieval, S3 Glacier Flexible Retrieval (formerly S3 Glacier), and S3 Glacier Deep Archive.
What is an S3 bucket name globally?	"<ul><li>Globally unique across all AWS accounts.</li>
</ul><div></div>"
What is S3 Transfer Acceleration?	A feature that enables fast, easy, and secure transfers of files over long distances between your client and your S3 bucket<sup></sup> by leveraging Amazon CloudFront's globally distributed edge locations.
What is S3 Object Versioning?	A feature that keeps multiple versions of an object in the same bucket, providing protection against accidental deletions and overwrites.
What are Access Control Lists (ACLs) in S3 used for?	To control who has access to buckets and objects. However, AWS recommends using Bucket Policies and IAM policies for access control.
What are Bucket Policies in S3?	Resource-based AWS Identity and Access Management (IAM) policies that you can attach to buckets to control access permissions.
What is the maximum size of a single object that can be stored in S3?	"5 terabytes.<br><br><div>It's also worth noting:</div><ul>
<li>The largest object that can be uploaded in a single <code>PUT</code> operation is 5 gigabytes (GB).<sup></sup> &nbsp; <div><div><div><div></div></div></div></div></li>
<li>For objects larger than 100 megabytes (MB), AWS recommends using the multipart upload capability, which is necessary for objects over 5 GB anyway</li></ul>"
"What does the acronym ""RRS"" stand for in the context of older S3 storage classes?"	Reduced Redundancy Storage (though this is less commonly used now and Standard-IA offers better availability and durability for a similar cost).
what is AWS RDS	<div><div><div>This experimental model uses your Search history. Some features aren't available.</div></div></div><div><div>AWS RDS stands for Amazon Relational Database Service.<sup></sup> It's a managed database service provided by AWS that makes it easier to set up, operate, and scale a relational database in the cloud.<sup></sup><sup></sup> It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.<sup></sup> &nbsp; <div><div><div><div></div><div></div><div></div></div></div></div></div><div>In simpler terms, if you need a database like MySQL, PostgreSQL, Oracle, SQL Server, or MariaDB, AWS RDS lets you run it without having to manage the underlying servers and infrastructure.<sup></sup> AWS takes care of that for you, so you can focus on your application and data.</div></div>
what is aws aurora	"<div>AWS Aurora is a fully managed, MySQL and PostgreSQL-compatible relational database service built for the cloud.<sup></sup> It combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness<sup></sup> of open-source databases.<sup></sup> &nbsp; <div><div><div><div></div><div></div><div></div></div></div></div></div><div>Here's a breakdown of what that means:</div><ul>
<li><strong>MySQL and PostgreSQL Compatible:</strong> This means that the code, applications, drivers, and tools you already use with MySQL or PostgreSQL can be used with Aurora with little to no modification.<sup></sup> This makes migration easier.<sup></sup> &nbsp; <div><div><div><div></div><div></div></div></div></div></li>
<li><strong>Performance and Availability:</strong> Aurora is designed for high performance, often outperforming standard MySQL and PostgreSQL databases.<sup></sup> It also offers built-in high availability features like automatic failover.<sup></sup> &nbsp; <div><div><div><div></div><div></div></div></div></div></li>
<li><strong>Managed Service:</strong> Like RDS, Aurora handles many of the database administration tasks, such as provisioning, patching, backups, recovery, and failure detection and repair.<sup></sup> &nbsp; <div><div><div><div></div></div></div></div></li>
<li><strong>Scalability:</strong> You can easily scale your database capacity up or down as your needs change.<sup></sup> &nbsp; <div><div><div><div></div></div></div></div></li>
<li><strong>Cost-Effective:</strong> While offering high performance and features, Aurora aims to be more cost-effective than traditional commercial databases.<sup></sup> &nbsp; <div><div><div><div></div></div></div></div></li>
</ul><div>Essentially, AWS Aurora is a proprietary relational database service from Amazon that is designed to be faster, more reliable, and easier to manage than running traditional MySQL or PostgreSQL on your own infrastructure or even on standard RDS.</div>"
What are common triggers for AWS Lambda functions?	S3 events, API Gateway requests, DynamoDB streams, SQS queues, SNS notifications, CloudWatch Events/EventBridge schedules, Cognito events, and more.
How is AWS Lambda priced?	Based on the number of requests for your functions and the duration (compute time in milliseconds) it takes for your code to execute, as well as the amount of memory allocated.
What is the maximum execution duration (timeout) for an AWS Lambda function?	15 minutes (900 seconds).
Name some programming languages supported by AWS Lambda.	"Node.js, Python, Java, C#, Go, Ruby, PowerShell. Custom runtimes can also be used."
What are Lambda Layers?	A way to centrally manage code and dependencies that are shared across multiple Lambda functions. They help reduce the size of uploaded deployment archives.
How does AWS Lambda handle scaling?	Lambda automatically scales out by running multiple instances of your function in parallel in response to the number of incoming events or requests.
What is the purpose of an IAM Role in AWS Lambda?	It grants the Lambda function permissions to access other AWS services and resources (e.g., read from S3, write to DynamoDB).
"What is a ""cold start"" in AWS Lambda?"	The latency experienced when a new instance of your Lambda function is initialized (downloading code, starting the runtime, initializing your code) before it can process an event.
"What is a ""warm start"" in AWS Lambda?"	When an already initialized instance of your Lambda function (which has recently processed an event) is available to handle a new incoming event, resulting in lower latency.
What is Provisioned Concurrency in AWS Lambda?	A feature that keeps a specified number of Lambda execution environments initialized and ready to respond immediately to incoming requests, helping to mitigate cold starts.
What is a Dead Letter Queue (DLQ) for AWS Lambda?	An SQS queue or SNS topic where Lambda can send events that it couldn't process successfully after retries. This helps in debugging and handling failed invocations.
How can you invoke a Lambda function directly via HTTPS without API Gateway?	Using Lambda Function URLs, which provide a dedicated HTTPS endpoint for your Lambda function.
What are Lambda Versions and Aliases used for?	"<strong>Versions</strong> are immutable snapshots of your function code and configuration. <strong>Aliases</strong> are pointers to specific versions (e.g., ""prod"" points to version 5), allowing for easier version management, rollbacks, and blue/green deployments."
if your lambda function is CPU heavy, how can you increase CPU	By increasing RAM, but only up to 1,792 MB. After that you have to multithread to get more compute because if you continue to increase RAM you will also be getting more cores
What is Amazon ECR	Elastic Container Registry<br>
ECS	Elastic Container Service<br>Amazon's own container platform
EKS	Elastic Kubernetes Service
Fargate	Amazons own serverless container platform<br>Works with ECS and EKS
What is AWS Step Functions?	A serverless orchestration service that lets you coordinate multiple AWS services into visual workflows, making it easier to build and manage complex applications and microservices.
Can AWS Step Functions run tasks in parallel?	<div>Yes, it supports parallel execution of tasks.</div><div></div>
List some primary use cases for AWS Step Functions.	"* Coordinating components of microservices.
<br>* Automating ETL processes.
<br>* Sequencing batch processing jobs.
<br>* Orchestrating data processing pipelines.
<br>* Implementing human approval workflows."
What is AWS AppSync?	A managed service that simplifies application development by letting you create a flexible GraphQL API to securely access, manipulate, and combine data from one or more data sources.
What kind of API does AWS AppSync primarily provide?	A managed GraphQL endpoint, enabling clients to request only the data they need.
"<strong>Q1:</strong> What best describes AWS Step Functions?
a) A managed GraphQL API service.
b) A service for building user interfaces for web and mobile apps. c) A serverless orchestration service for coordinating AWS services into visual workflows.
d) A service for deploying and hosting static web applications."	c) A serverless orchestration service for coordinating AWS services into visual workflows.&nbsp;
"In AWS Step Functions, what is the term for a defined workflow consisting of a series of steps?
a) A Lambda Invocation b) A State Machine
c) An Amplify Backend
d) An AppSync Resolver"	b) A State Machine
"Which of the following is a primary use case for AWS Step Functions?
a) Serving real-time data to mobile clients via GraphQL subscriptions. b) Automating ETL (Extract, Transform, Load) processes.
c) Building user authentication flows for web applications.
d) Storing large binary objects in the cloud."	b) Automating ETL (Extract, Transform, Load) processes.
"AWS Step Functions directly integrates with which of the following services to execute tasks?
a) Only AWS Lambda functions.
b) Only Amazon S3 and DynamoDB. c) Numerous AWS services like Lambda, SQS, SNS, DynamoDB, Batch, etc.
d) Primarily third-party SaaS solutions."	c) Numerous AWS services like Lambda, SQS, SNS, DynamoDB, Batch, etc.
"How does AWS Step Functions primarily help with complex application logic?
a) By providing SDKs for frontend development.
b) By offering a database with flexible schema. c) By allowing visual design, error handling, and retries for multi-step workflows.
d) By caching frequently accessed data at edge locations."	c) By allowing visual design, error handling, and retries for multi-step workflows.
"What is the core functionality of AWS AppSync?
a) To orchestrate serverless workflows using state machines. b) To create a managed GraphQL API for accessing and combining data from multiple sources.
c) To provide a command-line interface for managing AWS backend resources.
d) To host and manage CI/CD for web applications."	b) To create a managed GraphQL API for accessing and combining data from multiple sources.
"Which API technology is AWS AppSync primarily based on?
a) REST
b) SOAP c) GraphQL
d) gRPC"	c) GraphQL
"Which feature of AWS AppSync allows clients to receive real-time data updates?
a) Batch Resolvers
b) DataStore offline sync c) GraphQL Subscriptions via WebSockets
d) IAM Authorization"	c) GraphQL Subscriptions via WebSockets
"Which of the following is NOT a typical data source for AWS AppSync?
a) AWS Lambda
b) Amazon DynamoDB c) AWS Step Functions State Machines (as a direct data source type)
d) HTTP endpoints"	c) AWS Step Functions State Machines (as a direct data source type)
"For which scenario would AWS AppSync be a particularly strong choice?
a) Running long-duration batch processing jobs. b) Building a mobile application requiring selective data fetching and offline capabilities.
c) Managing user identities and authentication for a simple website.
d) Orchestrating a sequence of data transformation tasks."	b) Building a mobile application requiring selective data fetching and offline capabilities.
"What is AWS Amplify best described as?
a) A managed GraphQL service.
b) A serverless workflow orchestration tool. c) A development platform and framework for building full-stack web and mobile apps on AWS.
d) A relational database service."	c) A development platform and framework for building full-stack web and mobile apps on AWS.&nbsp;
"Which tool is a core part of AWS Amplify for creating and managing backend resources from your local development environment?
a) Amplify Visual Studio
b) AWS Management Console c) Amplify CLI (Command Line Interface)
d) AWS CloudFormation Designer"	c) Amplify CLI (Command Line Interface)&nbsp;
"AWS Amplify simplifies adding backend features like authentication. Which AWS service does it typically use for this?
a) AWS IAM
b) AWS Secrets Manager c) Amazon Cognito
d) AWS Shield"	c) Amazon Cognito
"Which of the following is a primary use case for AWS Amplify? a) Rapidly developing and deploying a full-stack serverless web application.
b) Managing complex data ETL pipelines.
c) Providing a managed NoSQL database with GraphQL access.
d) Orchestrating long-running, multi-step business processes."	a) Rapidly developing and deploying a full-stack serverless web application.
"What does ""Amplify Hosting"" primarily provide?
a) A managed Kubernetes cluster for containerized applications.
b) A serverless compute environment for running backend functions. c) Managed static web hosting with CI/CD capabilities.
d) A relational database instance for your application."	c) Managed static web hosting with CI/CD capabilities.
What is the primary purpose of AWS CloudFormation?<br>A) To store application code in a managed repository.<br>B) To provision and manage AWS infrastructure using code.<br>C) To monitor application performance and logs.<br>D) To provide a serverless execution environment for code.	<b>B) To provision and manage AWS infrastructure using code.</b><br>CloudFormation is a service that allows you to define your cloud infrastructure using code. It's a framework for deploying applications predictably [1].
Which of the following is a key benefit of using AWS CloudFormation for infrastructure management?<br>A) It automatically optimizes application code for better performance.<br>B) It allows version control and review of infrastructure changes through code.<br>C) It encrypts all data stored in AWS services by default.<br>D) It provides a built-in CI/CD pipeline for application deployments.	<b>B) It allows version control and review of infrastructure changes through code.</b><br>Benefits include Infrastructure as code, version control, and change review through code [1].
AWS CloudFormation helps with cost tracking because:<br>A) It automatically applies cost allocation tags to resources.<br>B) It only allows the creation of cost-optimized resources.<br>C) Each resource created within a stack is tagged, making cost tracking easier.<br>D) It provides real-time cost forecasting dashboards.	<b>C) Each resource created within a stack is tagged, making cost tracking easier.</b><br>Cost tracking is improved because each resource in a stack is tagged, making it easy to see the cost of a stack [1].
What are the two main languages supported for writing AWS CloudFormation templates?<br>A) XML and YAML<br>B) JSON and XML<br>C) YAML and JSON<br>D) Python and YAML	<b>C) YAML and JSON</b><br>YAML and JSON are the languages you can use for CloudFormation templates, with YAML generally preferred [2].
Which component is mandatory in every AWS CloudFormation template?<br>A) Parameters<br>B) Mappings<br>C) Resources<br>D) Outputs	<b>C) Resources</b><br>Resources are the core and mandatory component of your CloudFormation template, representing the AWS Components that will be created [2, 3].
How are AWS CloudFormation templates typically deployed or updated?<br>A) Templates are directly uploaded to the CloudFormation console and edited in place.<br>B) Templates are uploaded to S3 and then referenced by CloudFormation. Updates require re-uploading a new version to S3.<br>C) Templates are stored in AWS CodeCommit and automatically deployed when changes are pushed.<br>D) Templates are email attachments sent to the CloudFormation service endpoint.	<b>B) Templates are uploaded to S3 and then referenced by CloudFormation. Updates require re-uploading a new version to S3.</b><br>Templates must be uploaded in S3 and then referenced in CloudFormation. To update, you must re-upload a new version of the template to AWS [4].
What happens when an AWS CloudFormation stack is deleted?<br>A) Only the CloudFormation template is removed, leaving the created resources intact.<br>B) The stack is marked as inactive, but resources are preserved.<br>C) Every single artifact that was created by CloudFormation for that stack is deleted.<br>D) Resources are scaled down but not fully deleted.	<b>C) Every single artifact that was created by CloudFormation for that stack is deleted.</b><br>Deleting a stack deletes every single artifact that was created by CloudFormation [4].
What is the primary use case for the `Parameters` section in an AWS CloudFormation template?<br>A) Defining fixed variables within the template.<br>B) Declaring optional output values from the stack.<br>C) Providing dynamic inputs for the template that are user-specific.<br>D) Specifying conditions for resource creation.	<b>C) Providing dynamic inputs for the template that are user-specific.</b><br>Parameters are used for dynamic inputs for your template and are useful when values are user-specific [3, 5].
When would you prefer to use the `Mappings` section in an AWS CloudFormation template instead of `Parameters`?<br>A) When the values are user-specific and not known in advance.<br>B) When you need to reference values from resources created in other stacks.<br>C) When you need to define fixed variables that can be deduced from variables like Region or Environment.<br>D) When you need to define conditions for creating resources.	<b>C) When you need to define fixed variables that can be deduced from variables like Region or Environment.</b><br>Mappings are great when you know in advance all the values and they can be deduced from variables like Region or Environment. They are fixed variables within your template [5-7].
What is the main purpose of the `Outputs` section in a CloudFormation template?<br>A) To define parameters that users can input before stack creation.<br>B) To specify conditions under which resources should be created.<br>C) To declare values from the stack that can be viewed or imported by other stacks.<br>D) To map input values to fixed output values.	<b>C) To declare values from the stack that can be viewed or imported by other stacks.</b><br>The Outputs section declares optional output values that can be viewed in the console/CLI or imported into other stacks if exported [5, 8].
Which intrinsic function is used in CloudFormation to reference an output value that was exported from another CloudFormation stack?<br>A) `Fn::Ref`<br>B) `Fn::GetAtt`<br>C) `Fn::FindInMap`<br>D) `Fn::ImportValue`	<b>D) `Fn::ImportValue`</b><br>To leverage a value exported from one template within a second template, you use the `Fn::ImportValue` function [9].
What is the purpose of the `Conditions` section in an AWS CloudFormation template?<br>A) To define mapping variables based on environmental factors.<br>B) To control the creation of resources or outputs based on a condition.<br>C) To specify dynamic input parameters for the template.<br>D) To declare output values from the created resources.	<b>B) To control the creation of resources or outputs based on a condition.</b><br>Conditions are used to control the creation of resources or outputs based on a condition [9].
When using the `Fn::Ref` intrinsic function in CloudFormation, what does it return when referencing a Resource (like an EC2 instance)?<br>A) The ARN of the resource.<br>B) The physical ID of the underlying resource.<br>C) The logical ID of the resource defined in the template.<br>D) The value of a tag associated with the resource.	<b>B) The physical ID of the underlying resource.</b><br>When referencing a Resource, `Fn::Ref` returns the physical ID of the underlying resource (e.g., EC2 ID) [10].
Which CloudFormation intrinsic function allows you to retrieve an attribute (like the Availability Zone or Public IP) of a resource created within the template?<br>A) `Fn::Ref`<br>B) `Fn::GetAtt`<br>C) `Fn::FindInMap`<br>D) `Fn::Base64`	<b>B) `Fn::GetAtt`</b><br>`Fn::GetAtt` returns an attribute from a resource you create. You need to consult the documentation for available attributes [10].
Which intrinsic function is used to retrieve values from the `Mappings` section of a CloudFormation template?<br>A) `Fn::Ref`<br>B) `Fn::GetAtt`<br>C) `Fn::FindInMap`<br>D) `Fn::ImportValue`	<b>C) `Fn::FindInMap`</b><br>You use `Fn::FindInMap` to return a named value from a specific key within the Mappings section [7].
Which CloudFormation intrinsic function is useful for encoding data, such as passing data to an EC2 Instance's `UserData` property?<br>A) `Fn::Join`<br>B) `Fn::Sub`<br>C) `Fn::Base64`<br>D) `Fn::GetAtt`	<b>C) `Fn::Base64`</b><br>`Fn::Base64` converts a String to its Base64 representation and is used, for example, in the `UserData` property of an EC2 Instance [11].
Which of the following is a CloudFormation Pseudo Parameter available in any template?<br>A) `AWS::AccessKeyId`<br>B) `AWS::SecurityGroup`<br>C) `AWS::StackName`<br>D) `AWS::InstanceId`	<b>C) `AWS::StackName`</b><br>Important Pseudo Parameters include `AWS::AccountId`, `AWS::Region`, `AWS::StackId`, and `AWS::StackName` [6].
Why is YAML generally preferred over JSON for writing CloudFormation templates, according to the sources?<br>A) YAML supports more AWS resource types than JSON.<br>B) YAML allows for including comments and is more human-readable than JSON.<br>C) YAML templates are processed faster by CloudFormation.<br>D) JSON templates have a strict size limit that YAML does not.	<b>B) YAML allows for including comments and is more human-readable than JSON.</b><br>YAML is great in many ways, including supporting comments and being generally preferred over JSON for CloudFormation [2].
What is the recommended way to deploy CloudFormation templates for full automation?<br>A) Manually editing templates in the Infrastructure Composer and using the console.<br>B) Using the AWS CLI or a Continuous Delivery (CD) tool with YAML template files.<br>C) Uploading ZIP archives of templates via the AWS Management Console.<br>D) Executing SQL scripts to create AWS resources.	<b>B) Using the AWS CLI or a Continuous Delivery (CD) tool with YAML template files.</b><br>The automated way, using the AWS CLI or a Continuous Delivery (CD) tool with YAML files, is the recommended way for full automation [3].
CloudFormation supports declarative programming, which means:<br>A) You need to write code that explicitly defines the order in which resources are created.<br>B) You declare the desired state of your infrastructure, and CloudFormation figures out the necessary steps and ordering.<br>C) You use imperative programming languages like Python or Java to define your infrastructure.<br>D) You manually create resources, and CloudFormation documents them afterward.	<b>B) You declare the desired state of your infrastructure, and CloudFormation figures out the necessary steps and ordering.</b><br>CloudFormation offers declarative programming, meaning you don't need to figure out ordering and orchestration [1].
Which of the following database engines is NOT supported by Amazon RDS?<br>A) MySQL<br>B) PostgreSQL<br>C) MongoDB<br>D) Oracle	<b>C) MongoDB</b><br>Amazon RDS supports MySQL, PostgreSQL, MariaDB, Oracle, Microsoft SQL Server, and Amazon Aurora. MongoDB is a NoSQL database not supported by RDS.
What is the key difference between Amazon RDS and Amazon Aurora?<br>A) RDS only supports MySQL while Aurora supports multiple engines<br>B) Aurora is cloud-native and offers better performance and availability than RDS<br>C) RDS is serverless while Aurora requires instance management<br>D) Aurora only works in a single AZ while RDS supports Multi-AZ	<b>B) Aurora is cloud-native and offers better performance and availability than RDS</b><br>Aurora is AWS's cloud-native database offering up to 5x performance improvement over MySQL and 3x over PostgreSQL, with better availability features.
Amazon RDS Multi-AZ deployments provide:<br>A) Read scaling across multiple Availability Zones<br>B) Automatic failover to a standby instance in another AZ<br>C) Load balancing between multiple database instances<br>D) Automatic data partitioning across AZs	<b>B) Automatic failover to a standby instance in another AZ</b><br>Multi-AZ provides high availability with automatic failover to a standby replica in another AZ during planned maintenance or unplanned outages.
What are the two caching engines offered by Amazon ElastiCache?<br>A) MySQL and PostgreSQL<br>B) Redis and Memcached<br>C) DynamoDB and DocumentDB<br>D) Aurora and RDS	<b>B) Redis and Memcached</b><br>ElastiCache supports two popular open-source in-memory caching engines: Redis and Memcached, each with different features and use cases.
Which ElastiCache engine supports data persistence and complex data structures?<br>A) Memcached supports both features<br>B) Redis supports both features<br>C) Neither engine supports these features<br>D) Both engines support these features equally	<b>B) Redis supports both features</b><br>Redis supports data persistence (snapshots and AOF), complex data structures (lists, sets, sorted sets, hashes), and advanced features like pub/sub and transactions. Memcached is simpler and doesn't support persistence.
Amazon Aurora automatically backs up your database and retains backups for how long by default?<br>A) 1 day<br>B) 7 days<br>C) 30 days<br>D) 35 days	<b>B) 7 days</b><br>Aurora automatically backs up your database with a default retention period of 7 days, which can be extended up to 35 days. Backups are continuous and incremental.
What is the primary benefit of Amazon RDS Read Replicas?<br>A) Automatic failover during database outages<br>B) Scaling read workloads and reducing load on the primary database<br>C) Encrypting data at rest and in transit<br>D) Automating database schema migrations	<b>B) Scaling read workloads and reducing load on the primary database</b><br>Read Replicas allow you to create read-only copies of your database to scale read operations and reduce the load on your primary database instance.
Which AWS service provides the best solution for caching database query results to improve application performance?<br>A) Amazon RDS<br>B) Amazon Aurora<br>C) Amazon ElastiCache<br>D) Amazon DynamoDB	<b>C) Amazon ElastiCache</b><br>ElastiCache is specifically designed for caching frequently accessed data in memory, providing microsecond latency for read operations and significantly improving application performance.
Amazon Aurora Serverless is best suited for:<br>A) High-traffic production applications requiring consistent performance<br>B) Applications with unpredictable or intermittent workloads<br>C) Applications requiring complex database administration<br>D) Applications that need on-premises database connectivity	<b>B) Applications with unpredictable or intermittent workloads</b><br>Aurora Serverless automatically scales compute capacity based on application demand, making it ideal for intermittent, unpredictable workloads, or applications used infrequently.
What happens to your Amazon RDS instance during a maintenance window?<br>A) The instance continues running without any impact<br>B) The instance may experience a brief outage for single-AZ deployments<br>C) All data is automatically backed up to S3<br>D) The instance is permanently shut down	<b>B) The instance may experience a brief outage for single-AZ deployments</b><br>During maintenance windows, single-AZ deployments may experience brief outages while Multi-AZ deployments typically fail over to the standby instance to minimize downtime. 
What is the maximum execution timeout for an AWS Lambda function?<br>A) 5 minutes<br>B) 15 minutes<br>C) 30 minutes<br>D) 1 hour	<b>B) 15 minutes</b><br>AWS Lambda functions have a maximum execution timeout of 15 minutes (900 seconds). This is important for designing functions that complete within this limit.
What is the memory allocation range for AWS Lambda functions?<br>A) 128 MB to 1 GB<br>B) 128 MB to 10,240 MB<br>C) 256 MB to 3,008 MB<br>D) 512 MB to 10 GB	<b>B) 128 MB to&nbsp;</b>10,240<b>&nbsp;MB</b><br>Lambda functions can be allocated between 128 MB and 10,240&nbsp;MB of memory in 1 MB increments. CPU power scales proportionally with memory allocation.
Which of the following programming languages is NOT natively supported by AWS Lambda?<br>A) Python<br>B) Node.js<br>C) PHP<br>D) Go	<b>C) PHP</b><br>AWS Lambda natively supports Python, Node.js, Java, Go, .NET, and Ruby. PHP can run using custom runtimes or containers, but is not natively supported.
"What happens during a Lambda ""cold start""?<br>A) The function code is compressed and archived<br>B) AWS provisions a new execution environment and initializes the runtime<br>C) The function automatically scales down to zero instances<br>D) The function enters a hibernation state"	<b>B) AWS provisions a new execution environment and initializes the runtime</b><br>A cold start occurs when Lambda creates a new execution environment, loads the runtime, and initializes your function code, causing additional latency.
Which AWS service is commonly used to trigger Lambda functions based on file uploads?<br>A) Amazon EC2<br>B) Amazon S3<br>C) Amazon RDS<br>D) Amazon VPC	<b>B) Amazon S3</b><br>Amazon S3 can trigger Lambda functions automatically when objects are created, deleted, or modified in S3 buckets, making it a common event source.
What is the purpose of AWS Lambda Layers?<br>A) To increase the memory allocation of functions<br>B) To share code and dependencies across multiple functions<br>C) To encrypt function code at rest<br>D) To enable cross-region replication	<b>B) To share code and dependencies across multiple functions</b><br>Lambda Layers allow you to package and share code, libraries, and dependencies across multiple functions, reducing duplication and deployment package size.
How does AWS Lambda pricing work?<br>A) Fixed monthly fee per function<br>B) Charged per GB of memory allocated per month<br>C) Pay per request and compute time consumed<br>D) Charged per line of code executed	<b>C) Pay per request and compute time consumed</b><br>Lambda pricing is based on the number of requests and the compute time consumed (measured in GB-seconds), making it truly pay-per-use.
What is the maximum size limit for a Lambda deployment package when uploaded directly?<br>A) 10 MB<br>B) 50 MB<br>C) 100 MB<br>D) 250 MB	<b>B) 50 MB</b><br>The maximum deployment package size is 50 MB when uploaded directly (zipped). For larger packages, you must upload to S3 (up to 250 MB unzipped).
Which Lambda feature allows you to manage different versions of your function code?<br>A) Lambda Layers<br>B) Lambda Aliases<br>C) Lambda Versions and Aliases<br>D) Lambda Environment Variables	<b>C) Lambda Versions and Aliases</b><br>Lambda Versions create immutable snapshots of your function code, and Aliases provide mutable pointers to specific versions, enabling traffic splitting and blue/green deployments.
What is the default concurrent execution limit for AWS Lambda functions per region?<br>A) 100<br>B) 500<br>C) 1,000<br>D) 10,000	<b>C) 1,000</b><br>The default concurrent execution limit is 1,000 executions per region, though this can be increased by requesting a limit increase from AWS Support.
Which environment variable automatically provides the name of the Lambda function?<br>A) LAMBDA_FUNCTION_NAME<br>B) AWS_LAMBDA_FUNCTION_NAME<br>C) FUNCTION_NAME<br>D) AWS_FUNCTION_NAME	<b>B) AWS_LAMBDA_FUNCTION_NAME</b><br>AWS automatically sets the AWS_LAMBDA_FUNCTION_NAME environment variable with the name of the Lambda function being executed.
What is the purpose of a Dead Letter Queue (DLQ) in Lambda?<br>A) To store function logs for debugging<br>B) To cache frequently used data<br>C) To capture failed function invocations for analysis<br>D) To queue incoming requests during high traffic	<b>C) To capture failed function invocations for analysis</b><br>A Dead Letter Queue captures events that fail to be processed after the maximum retry attempts, allowing for error analysis and reprocessing.
Which AWS service provides the BEST way to monitor Lambda function performance and errors?<br>A) AWS Config<br>B) AWS CloudTrail<br>C) AWS CloudWatch<br>D) AWS X-Ray	<b>C) AWS CloudWatch</b><br>CloudWatch automatically collects Lambda metrics (duration, errors, throttles), logs, and enables custom metrics. X-Ray provides distributed tracing but CloudWatch is the primary monitoring service.
What happens when a Lambda function is invoked synchronously and returns an error?<br>A) The error is automatically retried 3 times<br>B) The error is sent to a Dead Letter Queue<br>C) The error is returned immediately to the caller<br>D) The function is automatically redeployed	<b>C) The error is returned immediately to the caller</b><br>For synchronous invocations, errors are returned directly to the caller. It's the caller's responsibility to handle retries. Automatic retries only occur for asynchronous invocations.
When should you use provisioned concurrency for Lambda functions?<br>A) To reduce costs for infrequently used functions<br>B) To eliminate cold starts for latency-sensitive applications<br>C) To increase the maximum execution timeout<br>D) To enable cross-region replication	<b>B) To eliminate cold starts for latency-sensitive applications</b><br>Provisioned concurrency keeps Lambda functions warm and ready to respond immediately, eliminating cold start latency for time-sensitive applications.
Which IAM component determines what AWS services a Lambda function can access?<br>A) Lambda Resource Policy<br>B) Lambda Execution Role<br>C) Lambda Function Policy<br>D) Lambda Security Group	<b>B) Lambda Execution Role</b><br>The Lambda execution role is an IAM role that grants the function permissions to access AWS services and resources during execution.
What is Lambda@Edge primarily used for?<br>A) Running Lambda functions in multiple AWS regions simultaneously<br>B) Executing Lambda functions at CloudFront edge locations<br>C) Connecting Lambda functions to on-premises networks<br>D) Scaling Lambda functions beyond normal concurrency limits	<b>B) Executing Lambda functions at CloudFront edge locations</b><br>Lambda@Edge allows you to run Lambda functions at CloudFront edge locations, enabling custom logic closer to users for improved performance.
Which event source can trigger a Lambda function to process records from a stream?<br>A) Amazon S3<br>B) Amazon API Gateway<br>C) Amazon Kinesis<br>D) Amazon Route 53	<b>C) Amazon Kinesis</b><br>Amazon Kinesis Data Streams can trigger Lambda functions to process streaming data records. Lambda polls the stream and invokes the function with batches of records.
What is the recommended approach for managing sensitive data like API keys in Lambda functions?<br>A) Hardcode them in the function code<br>B) Store them in environment variables<br>C) Use AWS Systems Manager Parameter Store or AWS Secrets Manager<br>D) Include them in the deployment package	<b>C) Use AWS Systems Manager Parameter Store or AWS Secrets Manager</b><br>Best practice is to store sensitive data in Parameter Store or Secrets Manager and retrieve it at runtime, rather than using environment variables which are visible in the console.
When a Lambda function is configured to run inside a VPC, what additional consideration is required for internet access?<br>A) The function automatically gets internet access<br>B) You must configure a NAT Gateway or NAT Instance for outbound internet access<br>C) Internet access is not possible from within a VPC<br>D) You need to attach an Elastic IP to the function	<b>B) You must configure a NAT Gateway or NAT Instance for outbound internet access</b><br>Lambda functions in a VPC only have access to resources within that VPC. For internet access, you need a NAT Gateway/Instance in a public subnet with proper route table configuration. 
What are the main types of Elastic Load Balancers offered by AWS?&lt;br&gt;A) Classic Load Balancer, Application Load Balancer, Gateway Load Balancer, Network Load Balancer&lt;br&gt;B) Application Load Balancer, Network Load Balancer, Classic Load Balancer&lt;br&gt;C) Network Load Balancer, Gateway Load Balancer, Regional Load Balancer&lt;br&gt;D) Classic Load Balancer, Regional Load Balancer, Global Load Balancer	&lt;b&gt;A) Classic Load Balancer, Application Load Balancer, Gateway Load Balancer, Network Load Balancer&lt;/b&gt;&lt;br&gt;AWS offers four main types of load balancers: Application Load Balancer (ALB) for HTTP/HTTPS, Network Load Balancer (NLB) for TCP/UDP/TLS, Gateway Load Balancer (GWLB) for third-party virtual appliances, and Classic Load Balancer (CLB) for legacy applications.
Which load balancer type operates at Layer 7 and can route traffic based on HTTP headers and URL paths?<br>A) Classic Load Balancer<br>B) Network Load Balancer<br>C) Application Load Balancer<br>D) Gateway Load Balancer	<b>C) Application Load Balancer</b><br>Application Load Balancer operates at Layer 7 (application layer) and can make routing decisions based on HTTP headers, URL paths, query parameters, and other application-level content.
What is the primary purpose of an Auto Scaling Group in AWS?<br>A) To automatically update EC2 instance software<br>B) To automatically adjust the number of EC2 instances based on demand<br>C) To automatically backup EC2 instance data<br>D) To automatically migrate instances between regions	<b>B) To automatically adjust the number of EC2 instances based on demand</b><br>Auto Scaling Groups automatically increase or decrease the number of EC2 instances in response to changing demand, helping maintain application availability and optimize costs.
What happens during an Auto Scaling Group health check failure?<br>A) The instance is automatically rebooted<br>B) The instance is marked unhealthy and terminated, then replaced<br>C) The instance is moved to a different Availability Zone<br>D) The instance receives additional CPU resources	<b>B) The instance is marked unhealthy and terminated, then replaced</b><br>When an Auto Scaling health check fails, the instance is marked as unhealthy, terminated, and automatically replaced with a new healthy instance to maintain the desired capacity.
Which scaling policy type allows you to scale based on a target value for a specific CloudWatch metric?<br>A) Step Scaling Policy<br>B) Simple Scaling Policy<br>C) Target Tracking Scaling Policy<br>D) Scheduled Scaling Policy	<b>C) Target Tracking Scaling Policy</b><br>Target Tracking Scaling Policy automatically adjusts capacity to maintain a target value for a specific metric (like CPU utilization at 50%), making it the simplest and most commonly used scaling policy type. 
What are the main Amazon S3 storage classes optimized for different access patterns?<br>A) Standard, Infrequent Access, Archive<br>B) Standard, Standard-IA, One Zone-IA, Glacier, Glacier Deep Archive<br>C) Hot, Warm, Cold, Frozen<br>D) Frequent, Infrequent, Rare, Never	<b>B) Standard, Standard-IA, One Zone-IA, Glacier, Glacier Deep Archive</b><br>S3 offers multiple storage classes: Standard (frequent access), Standard-IA (infrequent access), One Zone-IA (single AZ), Glacier (archival), and Glacier Deep Archive (long-term archival).
Which S3 security feature allows you to define who can access your bucket and what actions they can perform?<br>A) S3 Access Control Lists (ACLs)<br>B) S3 Bucket Policies<br>C) S3 User Policies<br>D) Both A and B	<b>D) Both A and B</b><br>Both S3 Bucket Policies (JSON-based) and Access Control Lists (ACLs) can control access to S3 buckets and objects, though bucket policies are more flexible and recommended.
What is the purpose of S3 Cross-Region Replication (CRR)?<br>A) To improve read performance by caching objects<br>B) To automatically replicate objects to a bucket in a different AWS region<br>C) To compress objects to save storage costs<br>D) To encrypt objects during transmission	<b>B) To automatically replicate objects to a bucket in a different AWS region</b><br>S3 Cross-Region Replication automatically replicates objects from a source bucket to a destination bucket in a different AWS region for compliance, disaster recovery, or latency reduction.
Which encryption option encrypts S3 objects using keys managed entirely by AWS?<br>A) SSE-C (Server-Side Encryption with Customer-Provided Keys)<br>B) SSE-KMS (Server-Side Encryption with AWS KMS)<br>C) SSE-S3 (Server-Side Encryption with Amazon S3-Managed Keys)<br>D) Client-Side Encryption	<b>C) SSE-S3 (Server-Side Encryption with Amazon S3-Managed Keys)</b><br>SSE-S3 uses encryption keys that are fully managed by Amazon S3, requiring no key management from the customer. AWS handles all encryption and decryption operations.
What is the maximum object size that can be uploaded to S3 in a single PUT operation?<br>A) 1 GB<br>B) 5 GB<br>C) 10 GB<br>D) 100 GB	<b>B) 5 GB</b><br>The maximum object size for a single PUT operation is 5 GB. For larger objects, you must use multipart upload, which allows objects up to 5 TB.
Which S3 feature allows you to provide temporary access to private S3 objects without requiring AWS credentials?<br>A) S3 Bucket Policies<br>B) S3 Access Control Lists<br>C) S3 Pre-signed URLs<br>D) S3 Transfer Acceleration	<b>C) S3 Pre-signed URLs</b><br>Pre-signed URLs allow temporary access to S3 objects using the credentials of the user who generated the URL, without requiring the recipient to have AWS credentials.
What happens when you enable S3 versioning on a bucket?<br>A) Objects are automatically compressed to save space<br>B) Multiple versions of the same object can be stored, with each version having a unique version ID<br>C) Objects are automatically encrypted<br>D) Objects are replicated to multiple regions	<b>B) Multiple versions of the same object can be stored, with each version having a unique version ID</b><br>S3 versioning allows multiple versions of an object to exist in the same bucket, helping protect against accidental deletion or modification.
Which S3 security feature helps prevent accidental deletion of objects and buckets?<br>A) S3 Encryption<br>B) S3 MFA Delete<br>C) S3 Access Logging<br>D) S3 Transfer Acceleration	<b>B) S3 MFA Delete</b><br>MFA Delete requires multi-factor authentication to permanently delete object versions or change versioning state, providing an additional security layer against accidental or malicious deletions.
What is the primary purpose of S3 lifecycle policies?<br>A) To encrypt objects automatically<br>B) To automatically transition objects between storage classes or delete them based on rules<br>C) To replicate objects to other regions<br>D) To compress objects to reduce storage costs	<b>B) To automatically transition objects between storage classes or delete them based on rules</b><br>S3 lifecycle policies automatically manage objects by transitioning them to cheaper storage classes over time or deleting them when they're no longer needed.
Which S3 access control method provides the most granular permission control and is recommended for most use cases?<br>A) S3 Access Control Lists (ACLs)<br>B) S3 Bucket Policies<br>C) IAM User Policies<br>D) S3 Public Access Settings	<b>B) S3 Bucket Policies</b><br>S3 Bucket Policies provide fine-grained access control using JSON policy language and are recommended over ACLs for most access control scenarios due to their flexibility and powerful condition support. 
What is the main difference between Amazon SQS Standard and FIFO queues?<br>A) Standard queues are faster, FIFO queues are more reliable<br>B) Standard queues offer at-least-once delivery, FIFO queues guarantee exactly-once delivery and message ordering<br>C) Standard queues are cheaper, FIFO queues support larger messages<br>D) Standard queues work globally, FIFO queues work in single regions	<b>B) Standard queues offer at-least-once delivery, FIFO queues guarantee exactly-once delivery and message ordering</b><br>Standard queues provide at-least-once delivery with best-effort ordering, while FIFO queues guarantee exactly-once processing and preserve message order.
What is the purpose of the visibility timeout in Amazon SQS?<br>A) To automatically delete old messages<br>B) To prevent other consumers from processing a message while it's being processed<br>C) To encrypt messages in transit<br>D) To limit the queue size	<b>B) To prevent other consumers from processing a message while it's being processed</b><br>Visibility timeout makes a message invisible to other consumers for a specified duration while one consumer processes it, preventing duplicate processing.
What is the maximum message retention period for Amazon SQS?<br>A) 7 days<br>B) 14 days<br>C) 30 days<br>D) 1 year	<b>B) 14 days</b><br>SQS can retain messages for up to 14 days. The default retention period is 4 days, but it can be configured from 1 minute to 14 days.
What is the primary purpose of Amazon SNS?<br>A) To queue messages for later processing<br>B) To send notifications to multiple subscribers simultaneously<br>C) To stream real-time data<br>D) To store large messages	<b>B) To send notifications to multiple subscribers simultaneously</b><br>SNS is a fully managed pub/sub messaging service that enables you to send notifications to multiple subscribers (fan-out pattern) simultaneously.
Which of the following protocols is NOT supported by Amazon SNS for message delivery?<br>A) HTTP/HTTPS<br>B) Email<br>C) SMS<br>D) FTP	<b>D) FTP</b><br>SNS supports HTTP/HTTPS, Email, SMS, SQS, Lambda, and mobile push notifications. FTP is not a supported delivery protocol.
What is the benefit of using SQS Dead Letter Queues (DLQ)?<br>A) To increase message processing speed<br>B) To encrypt sensitive messages<br>C) To capture messages that couldn't be processed successfully after multiple attempts<br>D) To prioritize important messages	<b>C) To capture messages that couldn't be processed successfully after multiple attempts</b><br>Dead Letter Queues capture messages that fail processing after a configured number of retry attempts, allowing for debugging and reprocessing.
What is the main difference between Amazon Kinesis Data Streams and Kinesis Data Firehose?<br>A) Data Streams is for real-time processing, Firehose is for delivery to data stores<br>B) Data Streams is cheaper, Firehose is more reliable<br>C) Data Streams supports larger data volumes<br>D) Data Streams works with structured data, Firehose with unstructured	<b>A) Data Streams is for real-time processing, Firehose is for delivery to data stores</b><br>Kinesis Data Streams enables real-time data processing with custom applications, while Firehose automatically delivers data to destinations like S3, Redshift, or Elasticsearch.
What is a shard in Amazon Kinesis Data Streams?<br>A) A backup copy of stream data<br>B) A unit of capacity that provides 1 MB/sec write and 2 MB/sec read throughput<br>C) An encryption key for the stream<br>D) A filter for incoming data	<b>B) A unit of capacity that provides 1 MB/sec write and 2 MB/sec read throughput</b><br>A shard is the base throughput unit of a Kinesis stream, providing 1 MB/sec or 1,000 records/sec write capacity and 2 MB/sec read capacity.
Which SQS feature helps reduce costs by eliminating empty responses when no messages are available?<br>A) Batch operations<br>B) Long polling<br>C) Message grouping<br>D) Compression	<b>B) Long polling</b><br>Long polling eliminates empty responses by waiting up to 20 seconds for messages to arrive, reducing the number of API calls and associated costs.
What happens when you send a message to an SNS topic that has no subscribers?<br>A) The message is stored until subscribers are added<br>B) The message is discarded<br>C) An error is returned<br>D) The message is sent to a dead letter queue	<b>B) The message is discarded</b><br>If an SNS topic has no subscribers when a message is published, the message is simply discarded. SNS does not store messages for later delivery.
What is the maximum message size for Amazon SQS?<br>A) 64 KB<br>B) 256 KB<br>C) 1 MB<br>D) 10 MB	<b>B) 256 KB</b><br>The maximum message payload size for SQS is 256 KB. For larger payloads, you can use the Extended Client Library to store data in S3 and send a reference.
Which Kinesis service would you use to transform and analyze streaming data in real-time using SQL?<br>A) Kinesis Data Streams<br>B) Kinesis Data Firehose<br>C) Kinesis Data Analytics<br>D) Kinesis Video Streams	<b>C) Kinesis Data Analytics</b><br>Kinesis Data Analytics allows you to process and analyze streaming data in real-time using SQL queries or Apache Flink applications.
What is the purpose of message filtering in Amazon SNS?<br>A) To encrypt messages before delivery<br>B) To allow subscribers to receive only messages that match specific attributes<br>C) To compress large messages<br>D) To prioritize message delivery	<b>B) To allow subscribers to receive only messages that match specific attributes</b><br>SNS message filtering allows subscribers to receive only the messages they're interested in by specifying filter policies based on message attributes.
What is the data retention period for Amazon Kinesis Data Streams by default?<br>A) 1 hour<br>B) 24 hours<br>C) 7 days<br>D) 30 days	<b>B) 24 hours</b><br>Kinesis Data Streams retains data for 24 hours by default, but this can be extended up to 365 days for additional cost.
Which pattern describes using SNS to send a message to multiple SQS queues simultaneously?<br>A) Request-Response Pattern<br>B) Fan-out Pattern<br>C) Load Balancing Pattern<br>D) Circuit Breaker Pattern	<b>B) Fan-out Pattern</b><br>The fan-out pattern uses SNS to distribute a single message to multiple SQS queues (or other endpoints) simultaneously, enabling parallel processing by different components. 
What is the primary purpose of AWS Step Functions?<br>A) To store application state data<br>B) To orchestrate and coordinate multiple AWS services using visual workflows<br>C) To cache frequently accessed data<br>D) To monitor application performance	<b>B) To orchestrate and coordinate multiple AWS services using visual workflows</b><br>Step Functions is a serverless orchestration service that lets you combine AWS services into serverless workflows using visual state machines.
What language is used to define Step Functions state machines?<br>A) JSON-based Amazon States Language (ASL)<br>B) YAML<br>C) Python<br>D) JavaScript	<b>A) JSON-based Amazon States Language (ASL)</b><br>Step Functions state machines are defined using Amazon States Language (ASL), a JSON-based declarative language for defining workflows.
What are the two types of Step Functions workflows?<br>A) Synchronous and Asynchronous<br>B) Standard and Express<br>C) Sequential and Parallel<br>D) Simple and Complex	<b>B) Standard and Express</b><br>Step Functions offers Standard workflows (long-running, exactly-once execution) and Express workflows (short-duration, high-volume, at-least-once execution).
Which Step Functions state type is used to make decisions based on input values?<br>A) Task State<br>B) Wait State<br>C) Choice State<br>D) Parallel State	<b>C) Choice State</b><br>Choice State adds branching logic to state machines by comparing input values against different conditions and transitioning to different states based on the results.
What is the primary purpose of AWS AppSync?<br>A) To synchronize data between mobile devices<br>B) To provide managed GraphQL APIs with real-time data synchronization<br>C) To backup application data<br>D) To monitor API performance	<b>B) To provide managed GraphQL APIs with real-time data synchronization</b><br>AppSync is a fully managed GraphQL service that enables real-time data synchronization and offline capabilities for mobile and web applications.
Which data sources can AWS AppSync connect to?<br>A) Only DynamoDB<br>B) DynamoDB, Lambda, ElasticSearch, and HTTP APIs<br>C) Only relational databases<br>D) Only Lambda functions	<b>B) DynamoDB, Lambda, ElasticSearch, and HTTP APIs</b><br>AppSync can connect to multiple data sources including DynamoDB, Lambda functions, ElasticSearch, HTTP APIs, and RDS via HTTP endpoints.
What is a resolver in AWS AppSync?<br>A) A caching mechanism for GraphQL queries<br>B) A function that connects GraphQL fields to data sources<br>C) A security policy for API access<br>D) A monitoring tool for API performance	<b>B) A function that connects GraphQL fields to data sources</b><br>Resolvers are functions that connect GraphQL schema fields to data sources, defining how to fetch or modify data when a field is requested.
Which Step Functions state allows you to execute multiple branches of work in parallel?<br>A) Task State<br>B) Choice State<br>C) Parallel State<br>D) Map State	<b>C) Parallel State</b><br>Parallel State executes multiple branches of work simultaneously, allowing you to run tasks in parallel and wait for all branches to complete before continuing.
What authentication methods does AWS AppSync support?<br>A) Only IAM<br>B) API Keys, AWS IAM, Amazon Cognito User Pools, and OpenID Connect<br>C) Only username and password<br>D) Only OAuth 2.0	<b>B) API Keys, AWS IAM, Amazon Cognito User Pools, and OpenID Connect</b><br>AppSync supports multiple authentication methods including API Keys, AWS IAM, Amazon Cognito User Pools, and OpenID Connect providers.
What happens when a Step Functions state machine execution fails?<br>A) The entire workflow is automatically retried<br>B) Execution stops and error details are available for debugging<br>C) The failed state is skipped and execution continues<br>D) The state machine is automatically deleted	<b>B) Execution stops and error details are available for debugging</b><br>When a Step Functions execution fails, it stops at the failed state and provides detailed error information, execution history, and visual representation of where the failure occurred for debugging. 
What is the primary purpose of Amazon CloudFront?<br>A) To store static website files<br>B) To provide a global content delivery network (CDN) that caches content at edge locations<br>C) To load balance traffic between EC2 instances<br>D) To encrypt data in transit	<b>B) To provide a global content delivery network (CDN) that caches content at edge locations</b><br>CloudFront is AWS's CDN service that caches content at edge locations worldwide to reduce latency and improve performance for end users.
What are CloudFront edge locations?<br>A) AWS data centers where your applications run<br>B) Geographic locations where CloudFront caches copies of your content<br>C) Security checkpoints for content validation<br>D) Backup storage locations for your data	<b>B) Geographic locations where CloudFront caches copies of your content</b><br>Edge locations are geographic points where CloudFront caches content closer to users, reducing latency and improving download speeds.
Which of the following can serve as an origin for a CloudFront distribution?<br>A) Only Amazon S3 buckets<br>B) Only EC2 instances<br>C) S3 buckets, custom HTTP servers, and Application Load Balancers<br>D) Only static websites	<b>C) S3 buckets, custom HTTP servers, and Application Load Balancers</b><br>CloudFront origins can be S3 buckets, custom HTTP/HTTPS servers (including EC2 instances), or Application Load Balancers, providing flexibility in content sources.
What is a CloudFront invalidation?<br>A) A security feature that blocks malicious requests<br>B) A method to immediately remove cached content from all edge locations<br>C) A way to compress content for faster delivery<br>D) A feature that encrypts cached content	<b>B) A method to immediately remove cached content from all edge locations</b><br>CloudFront invalidation allows you to remove objects from edge location caches before they expire, useful when you need to update content immediately across all locations.
What is an Origin Access Identity (OAI) in CloudFront?<br>A) A user account for accessing CloudFront settings<br>B) A special CloudFront user that can access S3 buckets, allowing you to restrict direct S3 access<br>C) An encryption key for securing content<br>D) A monitoring tool for tracking content access	<b>B) A special CloudFront user that can access S3 buckets, allowing you to restrict direct S3 access</b><br>OAI is a virtual user that CloudFront uses to access your S3 bucket content, enabling you to restrict direct access to S3 while allowing CloudFront to serve the content. 
What is the recommended way to provide AWS credentials to applications running on EC2 instances?<br>A) Hardcode AWS access keys in the application code<br>B) Store credentials in environment variables on the EC2 instance<br>C) Use IAM roles attached to the EC2 instance<br>D) Include credentials in configuration files	<b>C) Use IAM roles attached to the EC2 instance</b><br>IAM roles provide temporary, rotating credentials to EC2 instances automatically, eliminating the need to store long-term credentials and following security best practices.
What is the purpose of the AWS CLI (Command Line Interface)?<br>A) To provide a graphical interface for AWS services<br>B) To enable programmatic access to AWS services from the command line<br>C) To monitor AWS resource usage<br>D) To encrypt data in AWS services	<b>B) To enable programmatic access to AWS services from the command line</b><br>AWS CLI allows developers and administrators to interact with AWS services using commands, enabling automation, scripting, and programmatic access to AWS resources.
What is the difference between inline policies and managed policies in IAM?<br>A) Inline policies are encrypted, managed policies are not<br>B) Inline policies are attached directly to users/roles, managed policies are standalone and reusable<br>C) Inline policies work globally, managed policies work per region<br>D) Inline policies are free, managed policies have costs	<b>B) Inline policies are attached directly to users/roles, managed policies are standalone and reusable</b><br>Inline policies are embedded directly in users, groups, or roles, while managed policies are standalone policies that can be attached to multiple entities and are easier to manage.
What is the AWS SDK primarily used for?<br>A) Managing AWS infrastructure through a web interface<br>B) Integrating AWS services into applications using programming languages<br>C) Monitoring AWS service performance<br>D) Configuring network settings for AWS services	<b>B) Integrating AWS services into applications using programming languages</b><br>AWS SDKs provide libraries and APIs for various programming languages (Python, Java, JavaScript, etc.) to interact with AWS services programmatically within applications.
How do IAM roles differ from IAM users in terms of credentials?<br>A) Roles use permanent credentials, users use temporary credentials<br>B) Roles use temporary credentials that are automatically rotated, users have long-term access keys<br>C) Both use the same type of credentials<br>D) Roles don't use credentials, only users do	<b>B) Roles use temporary credentials that are automatically rotated, users have long-term access keys</b><br>IAM roles provide temporary, automatically rotating credentials that are assumed by applications or services, while IAM users have long-term access keys that must be manually managed. 
What is the primary purpose of Amazon ECR (Elastic Container Registry)?<br>A) To run containerized applications<br>B) To store and manage Docker container images<br>C) To orchestrate container deployments<br>D) To monitor container performance	<b>B) To store and manage Docker container images</b><br>ECR is a fully managed Docker container registry that makes it easy to store, manage, and deploy Docker container images securely and at scale.
What is the main difference between ECS EC2 launch type and ECS Fargate launch type?<br>A) EC2 is cheaper, Fargate is faster<br>B) EC2 requires you to manage the underlying infrastructure, Fargate is serverless<br>C) EC2 supports more container runtimes<br>D) Fargate only works with specific container sizes	<b>B) EC2 requires you to manage the underlying infrastructure, Fargate is serverless</b><br>With EC2 launch type, you manage the EC2 instances that run your containers. With Fargate, AWS manages the infrastructure and you only specify CPU and memory requirements.
What is an ECS Task Definition?<br>A) A running instance of a containerized application<br>B) A blueprint that describes how containers should run, including CPU, memory, and networking<br>C) A load balancer configuration for containers<br>D) A security policy for container access	<b>B) A blueprint that describes how containers should run, including CPU, memory, and networking</b><br>A Task Definition is like a blueprint that specifies which Docker images to use, CPU/memory requirements, networking mode, IAM roles, and logging configuration.
What is the difference between an ECS Task and an ECS Service?<br>A) Tasks are permanent, Services are temporary<br>B) Tasks are single instances, Services maintain a desired number of running tasks<br>C) Tasks use more memory than Services<br>D) Services can only run one container	<b>B) Tasks are single instances, Services maintain a desired number of running tasks</b><br>A Task is a single running instance of a Task Definition. A Service ensures a specified number of tasks are always running and can replace failed tasks automatically.
What authentication method does Docker use to access Amazon ECR?<br>A) Username and password<br>B) AWS IAM credentials and temporary tokens<br>C) SSH keys<br>D) API keys only	<b>B) AWS IAM credentials and temporary tokens</b><br>ECR uses AWS IAM for authentication. You must use 'aws ecr get-login-password' to get a temporary token that Docker uses to authenticate with ECR.
Which networking mode allows ECS containers to have their own elastic network interface (ENI)?<br>A) Bridge mode<br>B) Host mode<br>C) awsvpc mode<br>D) None mode	<b>C) awsvpc mode</b><br>The awsvpc network mode gives each task its own elastic network interface (ENI) with a private IP address, providing better network isolation and security.
What is the maximum number of containers you can define in a single ECS Task Definition?<br>A) 5<br>B) 10<br>C) 15<br>D) 20	<b>B) 10</b><br>An ECS Task Definition can contain up to 10 container definitions, allowing you to run multiple related containers as a single unit.
What happens when you delete a Docker image from ECR that is currently being used by running ECS tasks?<br>A) The running tasks immediately fail<br>B) The running tasks continue to run, but new tasks cannot be started<br>C) ECR prevents deletion of images in use<br>D) The image is automatically restored	<b>B) The running tasks continue to run, but new tasks cannot be started</b><br>Running containers continue to function because they already have the image locally, but new task instances cannot be started until the image is available again.
Which ECS feature automatically replaces failed tasks to maintain the desired number of running tasks?<br>A) Task Definition<br>B) ECS Cluster<br>C) ECS Service<br>D) Container Instance	<b>C) ECS Service</b><br>ECS Services monitor the health of tasks and automatically replace failed tasks to maintain the desired count, providing high availability for containerized applications.
What is required to run containers on ECS Fargate?<br>A) Pre-configured EC2 instances<br>B) Only Task Definition with CPU and memory specifications<br>C) Docker Swarm cluster<br>D) Kubernetes configuration	<b>B) Only Task Definition with CPU and memory specifications</b><br>Fargate is serverless, so you only need to specify your Task Definition with CPU and memory requirements. AWS handles all the underlying infrastructure automatically. 
What is the primary purpose of AWS Elastic Beanstalk?<br>A) To store application code in version control<br>B) To provide a platform for easily deploying and managing web applications without managing infrastructure<br>C) To monitor application performance<br>D) To encrypt application data	<b>B) To provide a platform for easily deploying and managing web applications without managing infrastructure</b><br>Elastic Beanstalk is a Platform-as-a-Service (PaaS) that handles deployment, monitoring, and scaling of web applications while you focus on writing code.
Which programming languages and platforms are supported by Elastic Beanstalk?<br>A) Only Java and Python<br>B) Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker<br>C) Only web-based languages<br>D) Any language that can run in a container	<b> it's actually d ... B) Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker</b><br>Elastic Beanstalk supports multiple platforms including Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker containers.
What is the difference between Blue/Green and Traffic Splitting deployment strategies in Elastic Beanstalk?<br>A) Blue/Green uses Route 53 DNS switching, Traffic Splitting uses ALB to gradually shift traffic<br>B) Blue/Green is faster, Traffic Splitting is safer<br>C) Blue/Green works with containers, Traffic Splitting works with VMs<br>D) They are the same strategy with different names	<b>A) Blue/Green uses Route 53 DNS switching, Traffic Splitting uses ALB to gradually shift traffic</b><br>Blue/Green deployment switches traffic via Route 53 DNS, while Traffic Splitting uses Application Load Balancer to gradually shift traffic between old and new application versions.
What happens during an Immutable deployment in Elastic Beanstalk?<br>A) Existing instances are updated in place<br>B) New instances are added to the existing Auto Scaling Group<br>C) A new Auto Scaling Group with new instances is created, then old instances are terminated<br>D) Only the application code is replaced	<b>C) A new Auto Scaling Group with new instances is created, then old instances are terminated</b><br>Immutable deployment creates entirely new instances in a new Auto Scaling Group and terminates the old instances after the new ones are healthy.
How does Elastic Beanstalk pricing work?<br>A) You pay a monthly fee for the Beanstalk service<br>B) You pay for Beanstalk plus the underlying AWS resources<br>C) You only pay for the underlying AWS resources (EC2, ELB, etc.), Beanstalk itself is free<br>D) Pricing is based on the number of deployments	<b>C) You only pay for the underlying AWS resources (EC2, ELB, etc.), Beanstalk itself is free</b><br>Elastic Beanstalk itself is free - you only pay for the underlying AWS resources like EC2 instances, load balancers, and auto scaling groups that it provisions.
What are the two types of Elastic Beanstalk environments?<br>A) Development and Production<br>B) Web Server Environment and Worker Environment<br>C) Single Instance and Load Balanced<br>D) Public and Private	<b>B) Web Server Environment and Worker Environment</b><br>Web Server Environments handle HTTP requests directly, while Worker Environments process background tasks from SQS queues.
What is the purpose of .ebextensions in Elastic Beanstalk?<br>A) To store application logs<br>B) To configure AWS resources and customize the environment beyond default settings<br>C) To compress application files<br>D) To monitor application health	<b>B) To configure AWS resources and customize the environment beyond default settings</b><br>.ebextensions are configuration files that allow you to customize your Elastic Beanstalk environment, install packages, modify settings, and configure AWS resources.
What is an Application Version in Elastic Beanstalk?<br>A) The version of the Elastic Beanstalk platform<br>B) A specific iteration of deployable code for your application<br>C) The version of the underlying operating system<br>D) A backup of your application data	<b>B) A specific iteration of deployable code for your application</b><br>An Application Version is a specific labeled iteration of your application code that can be deployed to one or more environments.
Which deployment strategy provides the fastest rollback capability in Elastic Beanstalk?<br>A) Rolling deployment<br>B) Rolling with additional batch<br>C) Blue/Green deployment<br>D) Immutable deployment	<b>C) Blue/Green deployment</b><br>Blue/Green deployment allows the fastest rollback because you can instantly switch back to the previous environment using Route 53 DNS switching.
How does Elastic Beanstalk monitor application health?<br>A) Only through CloudWatch metrics<br>B) Using health checks on a configurable URL path and CloudWatch monitoring<br>C) Manual monitoring only<br>D) Through application logs only	<b>B) Using health checks on a configurable URL path and CloudWatch monitoring</b><br>Elastic Beanstalk monitors health through configurable health check URLs (default is /) and integrates with CloudWatch for comprehensive monitoring and alerting. 
What is the primary purpose of AWS X-Ray?<br>A) To monitor server hardware metrics<br>B) To provide distributed tracing and analysis of applications<br>C) To store application logs<br>D) To scale applications automatically	<b>B) To provide distributed tracing and analysis of applications</b><br>X-Ray helps developers analyze and debug distributed applications by providing end-to-end tracing of requests across multiple services and identifying bottlenecks.
What are CloudWatch Metrics?<br>A) Log files generated by applications<br>B) Time-ordered data points that measure the performance of AWS services and applications<br>C) Configuration files for AWS resources<br>D) Security policies for resource access	<b>B) Time-ordered data points that measure the performance of AWS services and applications</b><br>CloudWatch Metrics are time-series data that provide insights into the performance and health of AWS resources and applications.
What is a CloudWatch Alarm?<br>A) A notification that AWS services are down<br>B) A monitoring rule that triggers actions when a metric crosses a specified threshold<br>C) An error message in application logs<br>D) A security alert for unauthorized access	<b>B) A monitoring rule that triggers actions when a metric crosses a specified threshold</b><br>CloudWatch Alarms monitor metrics and can trigger actions like SNS notifications, Auto Scaling, or EC2 actions when thresholds are breached.
What information does an X-Ray trace provide?<br>A) Only error messages from the application<br>B) A detailed map of request flow through services with timing and error information<br>C) Server configuration details<br>D) Database schema information	<b>B) A detailed map of request flow through services with timing and error information</b><br>X-Ray traces show the complete path of requests through your application, including service calls, response times, and errors at each step.
What are CloudWatch Custom Metrics?<br>A) Metrics automatically generated by AWS services<br>B) Application-specific metrics that you publish to CloudWatch using the API or CLI<br>C) Metrics from third-party monitoring tools<br>D) Historical data from previous months	<b>B) Application-specific metrics that you publish to CloudWatch using the API or CLI</b><br>Custom Metrics allow you to publish application-specific data points to CloudWatch using the PutMetricData API, enabling monitoring of business metrics.
What is the default retention period for CloudWatch Logs?<br>A) 7 days<br>B) 30 days<br>C) Never expires (indefinite)<br>D) 1 year	<b>C) Never expires (indefinite)</b><br>CloudWatch Logs are stored indefinitely by default, but you can set retention periods from 1 day to 10 years to manage costs and compliance requirements.
Which X-Ray component must be installed on EC2 instances to send trace data?<br>A) X-Ray SDK only<br>B) X-Ray Daemon only<br>C) Both X-Ray SDK and X-Ray Daemon<br>D) CloudWatch Agent	<b>C) Both X-Ray SDK and X-Ray Daemon</b><br>The X-Ray SDK instruments your application code to generate trace data, while the X-Ray Daemon collects and sends the trace data to the X-Ray service.
What is CloudWatch Logs Insights?<br>A) A real-time log streaming service<br>B) A query and analysis service for CloudWatch Logs using a SQL-like syntax<br>C) A log compression service<br>D) A log backup service	<b>B) A query and analysis service for CloudWatch Logs using a SQL-like syntax</b><br>CloudWatch Logs Insights allows you to interactively search and analyze log data using a query language, making it easy to identify patterns and troubleshoot issues.
What is the difference between CloudWatch detailed monitoring and basic monitoring for EC2?<br>A) Detailed monitoring provides more metric types<br>B) Detailed monitoring provides 1-minute intervals, basic monitoring provides 5-minute intervals<br>C) Detailed monitoring includes X-Ray tracing<br>D) Basic monitoring is more expensive	<b>B) Detailed monitoring provides 1-minute intervals, basic monitoring provides 5-minute intervals</b><br>Basic monitoring provides metrics at 5-minute intervals for free, while detailed monitoring provides metrics at 1-minute intervals for additional cost.
Which AWS service automatically integrates with X-Ray for distributed tracing?<br>A) EC2<br>B) Lambda<br>C) S3<br>D) RDS	<b>B) Lambda</b><br>AWS Lambda has built-in integration with X-Ray - you can enable tracing with a simple configuration change, and Lambda automatically captures traces for function executions and downstream service calls. 
What type of database is Amazon DynamoDB?<br>A) Relational SQL database<br>B) NoSQL key-value and document database<br>C) Graph database<br>D) Time-series database	<b>B) NoSQL key-value and document database</b><br>DynamoDB is a fully managed NoSQL database service that supports both key-value and document data models, providing fast and predictable performance.
What is a partition key in DynamoDB?<br>A) A foreign key that links tables<br>B) The primary attribute used to distribute data across partitions<br>C) An encrypted field for security<br>D) A backup identifier for data recovery	<b>B) The primary attribute used to distribute data across partitions</b><br>The partition key is used by DynamoDB's internal hash function to determine which partition to store the item in, enabling horizontal scaling.
What is the difference between DynamoDB's eventually consistent and strongly consistent reads?<br>A) Eventually consistent is faster but may return stale data, strongly consistent always returns the latest data<br>B) Eventually consistent is more expensive<br>C) Strongly consistent reads are not supported<br>D) There is no difference in DynamoDB	<b>A) Eventually consistent is faster but may return stale data, strongly consistent always returns the latest data</b><br>Eventually consistent reads (default) may not reflect recent writes but are faster and cheaper. Strongly consistent reads guarantee the latest data but consume more capacity.
What are the two capacity modes available for DynamoDB tables?<br>A) Standard and Express<br>B) Provisioned and On-Demand<br>C) Basic and Premium<br>D) Manual and Automatic	<b>B) Provisioned and On-Demand</b><br>Provisioned mode requires you to specify read/write capacity units in advance. On-Demand mode automatically scales and you pay per request without capacity planning.
What is a Global Secondary Index (GSI) in DynamoDB?<br>A) A backup copy of the table in another region<br>B) An index with a different partition key and optional sort key from the base table<br>C) A security feature for global access<br>D) A compression algorithm for large items	<b>B) An index with a different partition key and optional sort key from the base table</b><br>GSIs allow you to query data using different access patterns by creating alternate partition and sort key combinations, supporting additional query requirements.
What is the maximum item size in DynamoDB?<br>A) 64 KB<br>B) 256 KB<br>C) 400 KB<br>D) 1 MB	<b>C) 400 KB</b><br>Each DynamoDB item can be up to 400 KB in size, including attribute names and values. Items larger than this must be stored in S3 with references in DynamoDB.
What is DynamoDB Streams?<br>A) A real-time backup service<br>B) A feature that captures data modification events in a table<br>C) A query optimization tool<br>D) A data compression service	<b>B) A feature that captures data modification events in a table</b><br>DynamoDB Streams capture changes (inserts, updates, deletes) to items in a table, enabling real-time processing and triggering Lambda functions.
What is the difference between Query and Scan operations in DynamoDB?<br>A) Query searches by primary key, Scan examines every item in the table<br>B) Query is slower than Scan<br>C) Scan is more cost-effective<br>D) There is no difference	<b>A) Query searches by primary key, Scan examines every item in the table</b><br>Query operations are efficient and search using partition key (and optionally sort key). Scan operations examine every item in the table and are less efficient and more expensive.
What is DynamoDB DAX?<br>A) A data archiving service<br>B) An in-memory caching service that provides microsecond latency<br>C) A database migration tool<br>D) A monitoring service	<b>B) An in-memory caching service that provides microsecond latency</b><br>DynamoDB Accelerator (DAX) is a fully managed, in-memory cache for DynamoDB that delivers up to 10x performance improvement (microsecond latency).
What is Time to Live (TTL) in DynamoDB?<br>A) The maximum time a query can run<br>B) A feature that automatically deletes items after a specified timestamp<br>C) Connection timeout for clients<br>D) Backup retention period	<b>B) A feature that automatically deletes items after a specified timestamp</b><br>TTL allows you to define when items should be automatically deleted from your table, helping reduce storage costs and manage data lifecycle.
What are DynamoDB Global Tables?<br>A) Tables with global read access<br>B) Multi-region, multi-master replication for global applications<br>C) Tables larger than 1 TB<br>D) Publicly accessible tables	<b>B) Multi-region, multi-master replication for global applications</b><br>Global Tables provide multi-region, multi-master replication, allowing fast local reads and writes in multiple AWS regions with automatic synchronization.
What is a composite primary key in DynamoDB?<br>A) A key made of multiple data types<br>B) A combination of partition key and sort key<br>C) An encrypted primary key<br>D) A key shared across multiple tables	<b>B) A combination of partition key and sort key</b><br>A composite primary key consists of both a partition key and sort key, allowing multiple items with the same partition key but different sort keys.
What happens when you exceed the provisioned throughput capacity for a DynamoDB table?<br>A) The table automatically scales up<br>B) Requests are throttled and return ProvisionedThroughputExceededException<br>C) Additional charges are applied<br>D) The table becomes read-only	<b>B) Requests are throttled and return ProvisionedThroughputExceededException</b><br>When you exceed provisioned capacity, DynamoDB throttles requests and returns a ProvisionedThroughputExceededException. Applications should implement retry logic with exponential backoff.
What is a Local Secondary Index (LSI) in DynamoDB?<br>A) An index stored on the same partition as the base table<br>B) An index that spans all partitions<br>C) A backup index for disaster recovery<br>D) An index for local development only	<b>A) An index stored on the same partition as the base table</b><br>LSIs use the same partition key as the base table but a different sort key. They must be created at table creation time and share the partition's provisioned throughput.
Which DynamoDB feature helps optimize costs by automatically adjusting capacity based on traffic patterns?<br>A) DynamoDB Streams<br>B) Auto Scaling<br>C) Global Tables<br>D) DAX	<b>B) Auto Scaling</b><br>DynamoDB Auto Scaling automatically adjusts provisioned read and write capacity based on actual traffic patterns, helping optimize costs while maintaining performance. 
What is the primary purpose of AWS KMS (Key Management Service)?<br>A) To store and manage application secrets<br>B) To create, manage, and control cryptographic keys<br>C) To encrypt S3 buckets automatically<br>D) To monitor key usage	<b>B) To create, manage, and control cryptographic keys</b><br>AWS KMS is a managed service that makes it easy to create and control the encryption keys used to encrypt your data.
What is a Customer Master Key (CMK) in AWS KMS?<br>A) A key used to encrypt customer data directly<br>B) A logical representation of a master key that can encrypt/decrypt data up to 4KB or generate data keys<br>C) A key shared between AWS and the customer<br>D) A key used exclusively for S3 encryption	<b>B) A logical representation of a master key that can encrypt/decrypt data up to 4KB or generate data keys</b><br>CMKs are the primary resources in KMS. They are logical keys that can be used to encrypt or decrypt small amounts of data directly, or to generate, encrypt, and decrypt data keys.
What is Envelope Encryption?<br>A) Encrypting data multiple times with different keys<br>B) Encrypting the encryption key itself<br>C) Encrypting data with a data key, and then encrypting the data key with a master key (like a CMK)<br>D) Using a physical envelope to store keys	<b>C) Encrypting data with a data key, and then encrypting the data key with a master key (like a CMK)</b><br>Envelope encryption is a common practice where a data key is used to encrypt the data itself, and then that data key is encrypted by a master key (CMK in KMS). This protects the data key.
Which type of KMS CMK allows you to import your own key material?<br>A) AWS Managed CMK<br>B) Customer Managed CMK (with imported key material option)<br>C) CloudHSM CMK<br>D) AWS Owned CMK	<b>B) Customer Managed CMK (with imported key material option)</b><br>You can create a Customer Managed CMK without key material and then import your own key material, giving you more control over the key's origin.
What is the main difference between Symmetric and Asymmetric CMKs in KMS?<br>A) Symmetric keys are faster, Asymmetric keys are more secure<br>B) Symmetric keys use the same key for encryption and decryption; Asymmetric keys use a public key to encrypt and a private key to decrypt<br>C) Symmetric keys can be imported, Asymmetric keys cannot<br>D) Symmetric keys are only for S3, Asymmetric keys are for EC2	<b>B) Symmetric keys use the same key for encryption and decryption; Asymmetric keys use a public key to encrypt and a private key to decrypt</b><br>Symmetric CMKs use a single 256-bit key for both encryption and decryption. Asymmetric CMKs use mathematically related public and private key pairs.
How does AWS KMS integrate with IAM for access control?<br>A) KMS has its own separate access control system<br>B) Access to CMKs is controlled through Key Policies and optionally IAM policies<br>C) Only the root account can manage KMS keys<br>D) IAM roles cannot be used with KMS	<b>B) Access to CMKs is controlled through Key Policies and optionally IAM policies</b><br>Key policies are the primary way to control access to CMKs. IAM policies can be used in conjunction with key policies to grant permissions, but the key policy must allow it.
What is the purpose of a KMS Key Policy?<br>A) To define how often a key should be rotated<br>B) To specify which IAM users and roles can use or manage the CMK<br>C) To set the encryption algorithm for the key<br>D) To enable automatic key deletion	<b>B) To specify which IAM users and roles can use or manage the CMK</b><br>The key policy is a resource-based policy attached to a CMK that defines who has what permissions for that specific key.
What is automatic key rotation in AWS KMS?<br>A) Manually creating a new key and updating applications<br>B) A feature for Customer Managed CMKs where KMS automatically creates new cryptographic material for the CMK every year<br>C) Deleting old keys automatically<br>D) Changing the key alias periodically	<b>B) A feature for Customer Managed CMKs where KMS automatically creates new cryptographic material for the CMK every year</b><br>When enabled for a Customer Managed CMK, KMS generates new backing key material annually. The CMK's ARN and key ID remain the same, and KMS retains older versions to decrypt data previously encrypted.
Which AWS services can natively integrate with AWS KMS for server-side encryption?<br>A) Only S3 and EBS<br>B) S3, EBS, RDS, Lambda, and many others<br>C) Only services within a VPC<br>D) Only compute services like EC2 and Lambda	<b>B) S3, EBS, RDS, Lambda, and many others</b><br>Many AWS services integrate with KMS to provide server-side encryption using CMKs you manage or AWS managed CMKs, including S3, EBS, RDS, DynamoDB, Lambda, SQS, SNS, etc. 
